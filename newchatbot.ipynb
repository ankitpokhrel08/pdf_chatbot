{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for URL handling, warnings, file path manipulation, and pretty printing\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p  # Import Path class for file and directory manipulation\n",
    "from pprint import pprint  # For printing data structures in a readable format\n",
    "\n",
    "# Importing libraries for data handling and machine learning\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from langchain import PromptTemplate  # For creating prompt templates in LangChain\n",
    "from langchain.chains.question_answering import load_qa_chain  # To load a Question Answering chain\n",
    "from langchain.document_loaders import PyPDFLoader  # To load and process PDF documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # To split text into manageable chunks\n",
    "from langchain.vectorstores import Chroma  # For managing and querying vector-based document embeddings\n",
    "from langchain.chains import RetrievalQA  # To create a question-answering chain with a retriever\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Note: Restart the Python kernel if issues arise with LangChain imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=GOOGLE_API_KEY,\n",
    "                             temperature=0.2,convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_loader = PyPDFLoader(\"./pdfs/micro.pdf\")\n",
    "pages = pdf_loader.load_and_split()\n",
    "# print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Pipeline: Embedding + Gemini (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    model,\n",
    "    retriever=vector_index,\n",
    "    return_source_documents=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. Always use human face reaction emoji at the end of converstaion\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    model,\n",
    "    retriever=vector_index,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the `ADD` instruction for 8-bit numbers and `DAD` for 16-bit numbers.  For BCD numbers, use `ADD` followed by `DAA`. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "question = \"How to add two numbers in 8085\"\n",
    "result = qa_chain({\"query\": question})\n",
    "answer = result[\"result\"]\n",
    "print(answer,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the `ADD` instruction for 8-bit numbers and `DAD` for 16-bit numbers.  For BCD numbers, use `ADD` followed by `DAA`. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "question = \"How to add two numbers in 8085\"\n",
    "result = qa_chain({\"query\": question})\n",
    "answer = result[\"result\"]\n",
    "\n",
    "for line in answer.splitlines():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
